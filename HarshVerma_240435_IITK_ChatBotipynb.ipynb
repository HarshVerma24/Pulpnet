{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tp5mHrrouNwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469112bf-d3f0-426e-f9e9-a7d3fbf5b522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Enhanced IITK Data Scraping...\n",
            "This will scrape:\n",
            "- Official IITK websites\n",
            "- Student organizations: Vox Populi, E-Cell, Gymkhana, AnC Council\n",
            "- Student Placement Office (SPO)\n",
            "- Department pages\n",
            "- And follow relevant links from all sources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error scraping https://voxiitk.com/category/administration/: 404 Client Error: Not Found for url: https://voxiitk.com/category/administration/\n",
            "ERROR:__main__:Error scraping https://voxiitk.com/about-us/: 404 Client Error: Not Found for url: https://voxiitk.com/about-us/\n",
            "ERROR:__main__:Error scraping https://voxiitk.com/blog/: 404 Client Error: Not Found for url: https://voxiitk.com/blog/\n",
            "ERROR:__main__:Error scraping https://spo.iitk.ac.in/statistics: 404 Client Error: Not Found for url: https://spo.iitk.ac.in/statistics\n",
            "ERROR:__main__:Error scraping https://www.ecelliitk.org/about: 404 Client Error: Not Found for url: https://www.ecelliitk.org/about\n",
            "ERROR:__main__:Error scraping https://www.ecelliitk.org/events: 404 Client Error: Not Found for url: https://www.ecelliitk.org/events\n",
            "ERROR:__main__:Error scraping https://www.ecelliitk.org/startups: 404 Client Error: Not Found for url: https://www.ecelliitk.org/startups\n",
            "ERROR:__main__:Error scraping https://www.ecelliitk.org/team: 404 Client Error: Not Found for url: https://www.ecelliitk.org/team\n",
            "ERROR:__main__:Error scraping https://www.ecelliitk.org/blog: 404 Client Error: Not Found for url: https://www.ecelliitk.org/blog\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/gymkhana/: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: /gymkhana/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c4861e190>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/gymkhana/about: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: /gymkhana/about (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c48423990>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/gymkhana/councils: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: /gymkhana/councils (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c484a7d50>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/gymkhana/cells: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: /gymkhana/cells (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c48478ed0>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/gymkhana/festivals: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: /gymkhana/festivals (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c484a60d0>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n",
            "ERROR:__main__:Error scraping https://www.anciitk.co.in/about: 404 Client Error: Not Found for url: https://www.anciitk.co.in/about\n",
            "ERROR:__main__:Error scraping https://www.anciitk.co.in/events: 404 Client Error: Not Found for url: https://www.anciitk.co.in/events\n",
            "ERROR:__main__:Error scraping https://www.anciitk.co.in/resources: 404 Client Error: Not Found for url: https://www.anciitk.co.in/resources\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c4865be90>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n",
            "ERROR:__main__:Error scraping https://students.iitk.ac.in/gymkhana/: HTTPSConnectionPool(host='students.iitk.ac.in', port=443): Max retries exceeded with url: /gymkhana/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x795c486c7dd0>: Failed to resolve 'students.iitk.ac.in' ([Errno -2] Name or service not known)\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ENHANCED DATA COLLECTION SUMMARY ===\n",
            "Total pages scraped: 31\n",
            "Total words: 51,799\n",
            "Average words per page: 1670.9\n",
            "Unique URLs visited: 34\n",
            "\n",
            "=== BREAKDOWN BY SOURCE TYPE ===\n",
            "Student Org: 5 pages, 2,678 words\n",
            "Placement: 5 pages, 7,843 words\n",
            "Official: 21 pages, 41,278 words\n",
            "\n",
            "=== FILES CREATED ===\n",
            "Main JSON: enhanced_iitk_data.json\n",
            "Text version: enhanced_iitk_data_text.txt\n",
            "Student Org JSON: student_org_data.json\n",
            "Placement JSON: placement_data.json\n",
            "Official JSON: official_data.json\n",
            "\n",
            "Scraping completed successfully!\n",
            "\n",
            "Sample of scraped content by source type:\n",
            "\n",
            "Student Org:\n",
            "  Title: All about IITK  Vox Populi\n",
            "  URL: https://voxiitk.com/category/all-about-iitk/\n",
            "  Words: 200\n",
            "  Preview: Disclaimer: Vox Populi, IIT Kanpur, is the exclusive owner of the information on this website. No part of this content Disclaimer: Vox Populi, IIT Kan...\n",
            "\n",
            "Placement:\n",
            "  Title: Students' Placement Office, IIT Kanpur\n",
            "  URL: https://spo.iitk.ac.in/\n",
            "  Words: 864\n",
            "  Preview: About IITK For companies For students Samvardhan Contact IIT Kanpur Students' Placement Office The Students' Placement Office (SPO), IIT Kanpur is mai...\n",
            "\n",
            "Official:\n",
            "  Title: IIT Kanpur\n",
            "  URL: https://www.iitk.ac.in/\n",
            "  Words: 1387\n",
            "  Preview: Institute Special Recruitment Drive (DF-1/2025)  Rolling Advertisement 2025 Postdoctoral Fellows Apply Online Overview Education at IITK Academics at ...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse, quote\n",
        "import os\n",
        "from typing import List, Dict, Set\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EnhancedIITKDataScraper:\n",
        "    def __init__(self):\n",
        "        # Official IITK website URLs\n",
        "        self.main_urls = [\n",
        "            \"https://www.iitk.ac.in/\",\n",
        "            \"https://www.iitk.ac.in/new/\",\n",
        "            \"https://www.iitk.ac.in/doaa/\",\n",
        "            \"https://www.iitk.ac.in/dora/\",\n",
        "        ]\n",
        "\n",
        "        # Student organization websites (separate domains)\n",
        "        self.student_org_urls = [\n",
        "            \"https://voxiitk.com/\",\n",
        "            \"https://spo.iitk.ac.in/\",\n",
        "            \"https://www.ecelliitk.org/\",\n",
        "            \"https://students.iitk.ac.in/gymkhana/\",\n",
        "            \"https://www.anciitk.co.in/\",\n",
        "        ]\n",
        "\n",
        "        # Department URLs\n",
        "        self.department_urls = [\n",
        "            \"https://www.iitk.ac.in/me/\",\n",
        "            \"https://www.iitk.ac.in/me/about-us\",\n",
        "            \"https://www.iitk.ac.in/doaa/academic-departments\",\n",
        "            \"https://www.iitk.ac.in/doaa/pg-manual\",\n",
        "            \"https://www.iitk.ac.in/doaa/convocation\",\n",
        "            \"https://cer.iitk.ac.in/\",\n",
        "        ]\n",
        "\n",
        "        # Content URLs\n",
        "        self.content_urls = [\n",
        "            \"https://www.iitk.ac.in/new/research-overview\",\n",
        "            \"https://www.iitk.ac.in/new/admissions\",\n",
        "            \"https://students.iitk.ac.in/\",\n",
        "        ]\n",
        "\n",
        "        # Department codes to try\n",
        "        self.dept_codes = [\n",
        "            'ae', 'bsbe', 'ce', 'che', 'cse', 'ee', 'eco', 'hss',\n",
        "            'mse', 'math', 'me', 'mth', 'phy', 'stats', 'des', 'doms'\n",
        "        ]\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "        self.scraped_urls: Set[str] = set()\n",
        "        self.scraped_data = []\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text content\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove only problematic characters, keep useful punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\:\\;\\(\\)\\[\\]\\'\\\"\\&\\%\\$\\@\\+\\=\\/\\\\]', '', text)\n",
        "        # Remove repeated punctuation\n",
        "        text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def is_valid_url(self, url: str, base_domain: str = None) -> bool:\n",
        "        \"\"\"Check if URL is valid and from allowed domains\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(url)\n",
        "\n",
        "            # List of allowed domains\n",
        "            allowed_domains = [\n",
        "                'iitk.ac.in',\n",
        "                'voxiitk.com',\n",
        "                'spo.iitk.ac.in',\n",
        "                'ecelliitk.org',\n",
        "                'students.iitk.ac.in',\n",
        "                'anciitk.co.in'\n",
        "            ]\n",
        "\n",
        "            # Check if URL is from allowed domains\n",
        "            is_allowed_domain = any(domain in parsed.netloc for domain in allowed_domains)\n",
        "\n",
        "            # If base_domain is specified, prioritize same domain\n",
        "            if base_domain and base_domain in parsed.netloc:\n",
        "                is_allowed_domain = True\n",
        "\n",
        "            return (\n",
        "                parsed.scheme in ['http', 'https'] and\n",
        "                is_allowed_domain and\n",
        "                not any(ext in url.lower() for ext in ['.pdf', '.doc', '.docx', '.ppt', '.pptx', '.jpg', '.png', '.gif', '.zip', '.mp4', '.mp3'])\n",
        "            )\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def scrape_page(self, url: str) -> Dict:\n",
        "        \"\"\"Scrape a single page and extract relevant content\"\"\"\n",
        "        if url in self.scraped_urls:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Add to scraped URLs\n",
        "            self.scraped_urls.add(url)\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract title\n",
        "            title = soup.find('title')\n",
        "            title = title.get_text() if title else \"No Title\"\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \"noscript\", \"form\"]):\n",
        "                element.decompose()\n",
        "\n",
        "            # Try multiple strategies to extract main content\n",
        "            content = \"\"\n",
        "\n",
        "            # Strategy 1: Look for main content containers\n",
        "            main_selectors = [\n",
        "                'main', 'article', '.content', '.main-content',\n",
        "                '.post-content', '.entry-content', '.page-content',\n",
        "                '.content-area', '.site-content', 'div.content',\n",
        "                '.container', '.wrapper', '#content', '#main',\n",
        "                '.main-container', '.page-wrapper', '.post',\n",
        "                '.blog-post', '.article-content'\n",
        "            ]\n",
        "\n",
        "            for selector in main_selectors:\n",
        "                content_elem = soup.select_one(selector)\n",
        "                if content_elem:\n",
        "                    content = content_elem.get_text(separator=' ', strip=True)\n",
        "                    break\n",
        "\n",
        "            # Strategy 2: For Vox Populi and other blog-style sites\n",
        "            if not content or len(content.split()) < 20:\n",
        "                # Look for blog post content\n",
        "                blog_selectors = [\n",
        "                    '.single-post', '.post-entry', '.entry', '.blog-content',\n",
        "                    '.wp-content', '.post-body', '.article-body'\n",
        "                ]\n",
        "                for selector in blog_selectors:\n",
        "                    content_elem = soup.select_one(selector)\n",
        "                    if content_elem:\n",
        "                        content = content_elem.get_text(separator=' ', strip=True)\n",
        "                        break\n",
        "\n",
        "            # Strategy 3: If no main content, look for specific content divs\n",
        "            if not content or len(content.split()) < 20:\n",
        "                content_divs = soup.find_all('div', class_=re.compile(r'content|main|article|post|text|body'))\n",
        "                if content_divs:\n",
        "                    content = ' '.join([div.get_text(separator=' ', strip=True) for div in content_divs])\n",
        "\n",
        "            # Strategy 4: Extract from body but filter out navigation\n",
        "            if not content or len(content.split()) < 20:\n",
        "                body = soup.find('body')\n",
        "                if body:\n",
        "                    # Remove navigation elements\n",
        "                    for nav_elem in body.find_all(['nav', 'menu', 'sidebar']):\n",
        "                        nav_elem.decompose()\n",
        "\n",
        "                    # Remove lists that look like navigation\n",
        "                    for ul_elem in body.find_all(['ul', 'ol']):\n",
        "                        if ul_elem.get('class'):\n",
        "                            nav_classes = ' '.join(ul_elem.get('class', []))\n",
        "                            if any(nav_word in nav_classes.lower() for nav_word in ['nav', 'menu', 'sidebar', 'breadcrumb']):\n",
        "                                ul_elem.decompose()\n",
        "\n",
        "                    content = body.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Strategy 5: Get all paragraph content\n",
        "            if not content or len(content.split()) < 20:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                if paragraphs:\n",
        "                    content = ' '.join([p.get_text(separator=' ', strip=True) for p in paragraphs])\n",
        "\n",
        "            # Clean the content\n",
        "            content = self.clean_text(content)\n",
        "\n",
        "            # Extract headings and structure\n",
        "            sections = []\n",
        "            headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "            for heading in headings:\n",
        "                heading_text = self.clean_text(heading.get_text())\n",
        "                if heading_text and len(heading_text) > 3:\n",
        "                    sections.append(heading_text)\n",
        "\n",
        "            # Extract meta description\n",
        "            meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "            description = meta_desc.get('content') if meta_desc else \"\"\n",
        "\n",
        "            # Extract relevant links for further crawling\n",
        "            links = []\n",
        "            base_domain = urlparse(url).netloc\n",
        "\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link.get('href')\n",
        "                if href and not href.startswith('#') and not href.startswith('mailto:'):\n",
        "                    full_url = urljoin(url, href)\n",
        "                    if self.is_valid_url(full_url, base_domain) and full_url not in self.scraped_urls:\n",
        "                        # Prioritize certain types of links\n",
        "                        link_text = link.get_text().lower()\n",
        "                        if any(keyword in link_text for keyword in ['about', 'department', 'faculty', 'research', 'academic', 'course', 'program', 'article', 'post', 'news', 'blog']):\n",
        "                            links.insert(0, full_url)  # Add to front\n",
        "                        else:\n",
        "                            links.append(full_url)\n",
        "\n",
        "            word_count = len(content.split())\n",
        "\n",
        "            # Only return if we have substantial content\n",
        "            if word_count < 25:\n",
        "                return None\n",
        "\n",
        "            # Determine source type\n",
        "            source_type = \"official\"\n",
        "            if any(domain in url for domain in ['voxiitk.com', 'ecelliitk.org', 'anciitk.co.in']):\n",
        "                source_type = \"student_org\"\n",
        "            elif 'spo.iitk.ac.in' in url:\n",
        "                source_type = \"placement\"\n",
        "            elif 'students.iitk.ac.in' in url:\n",
        "                source_type = \"student_portal\"\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'title': self.clean_text(title),\n",
        "                'content': content,\n",
        "                'description': self.clean_text(description),\n",
        "                'sections': sections,\n",
        "                'word_count': word_count,\n",
        "                'source_type': source_type,\n",
        "                'links': links[:10]  # Limit to prevent explosion\n",
        "            }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error scraping {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def scrape_student_organizations(self) -> List[Dict]:\n",
        "        \"\"\"Scrape student organization websites\"\"\"\n",
        "        org_data = []\n",
        "\n",
        "        for base_url in self.student_org_urls:\n",
        "            logger.info(f\"Scraping student organization: {base_url}\")\n",
        "\n",
        "            # Scrape main page\n",
        "            page_data = self.scrape_page(base_url)\n",
        "            if page_data:\n",
        "                org_data.append(page_data)\n",
        "\n",
        "            # For each organization, try to find additional pages\n",
        "            additional_pages = []\n",
        "\n",
        "            if 'voxiitk.com' in base_url:\n",
        "                # Vox Populi specific pages\n",
        "                vox_pages = [\n",
        "                    \"https://voxiitk.com/category/all-about-iitk/\",\n",
        "                    \"https://voxiitk.com/category/flagship-series/as-we-leave/\",\n",
        "                    \"https://voxiitk.com/category/reports-and-investigations/\",\n",
        "                    \"https://voxiitk.com/category/administration/\",\n",
        "                    \"https://voxiitk.com/about-us/\",\n",
        "                    \"https://voxiitk.com/blog/\",\n",
        "                ]\n",
        "                additional_pages.extend(vox_pages)\n",
        "\n",
        "            elif 'spo.iitk.ac.in' in base_url:\n",
        "                # SPO specific pages\n",
        "                spo_pages = [\n",
        "                    \"https://spo.iitk.ac.in/insights\",\n",
        "                    \"https://spo.iitk.ac.in/companies\",\n",
        "                    \"https://spo.iitk.ac.in/students\",\n",
        "                    \"https://spo.iitk.ac.in/about\",\n",
        "                    \"https://spo.iitk.ac.in/statistics\",\n",
        "                ]\n",
        "                additional_pages.extend(spo_pages)\n",
        "\n",
        "            elif 'ecelliitk.org' in base_url:\n",
        "                # E-Cell specific pages\n",
        "                ecell_pages = [\n",
        "                    \"https://www.ecelliitk.org/about\",\n",
        "                    \"https://www.ecelliitk.org/events\",\n",
        "                    \"https://www.ecelliitk.org/startups\",\n",
        "                    \"https://www.ecelliitk.org/team\",\n",
        "                    \"https://www.ecelliitk.org/blog\",\n",
        "                ]\n",
        "                additional_pages.extend(ecell_pages)\n",
        "\n",
        "            elif 'students.iitk.ac.in' in base_url:\n",
        "                # Gymkhana specific pages\n",
        "                gymkhana_pages = [\n",
        "                    \"https://students.iitk.ac.in/gymkhana/about\",\n",
        "                    \"https://students.iitk.ac.in/gymkhana/councils\",\n",
        "                    \"https://students.iitk.ac.in/gymkhana/cells\",\n",
        "                    \"https://students.iitk.ac.in/gymkhana/festivals\",\n",
        "                ]\n",
        "                additional_pages.extend(gymkhana_pages)\n",
        "\n",
        "            elif 'anciitk.co.in' in base_url:\n",
        "                # AnC Council specific pages\n",
        "                anc_pages = [\n",
        "                    \"https://www.anciitk.co.in/about\",\n",
        "                    \"https://www.anciitk.co.in/team\",\n",
        "                    \"https://www.anciitk.co.in/events\",\n",
        "                    \"https://www.anciitk.co.in/resources\",\n",
        "                ]\n",
        "                additional_pages.extend(anc_pages)\n",
        "\n",
        "            # Scrape additional pages\n",
        "            for page_url in additional_pages:\n",
        "                if page_url not in self.scraped_urls:\n",
        "                    page_data = self.scrape_page(page_url)\n",
        "                    if page_data:\n",
        "                        org_data.append(page_data)\n",
        "                    time.sleep(1)\n",
        "\n",
        "            time.sleep(2)  # Longer delay between organizations\n",
        "\n",
        "        return org_data\n",
        "\n",
        "    def discover_department_urls(self) -> List[str]:\n",
        "        \"\"\"Discover working department URLs\"\"\"\n",
        "        department_urls = []\n",
        "\n",
        "        # Try common department patterns\n",
        "        for dept in self.dept_codes:\n",
        "            urls_to_try = [\n",
        "                f\"https://www.iitk.ac.in/{dept}/\",\n",
        "                f\"https://www.iitk.ac.in/{dept}/about\",\n",
        "                f\"https://www.iitk.ac.in/{dept}/about-us\",\n",
        "                f\"https://www.iitk.ac.in/{dept}/faculty\",\n",
        "                f\"https://www.iitk.ac.in/{dept}/research\",\n",
        "                f\"https://www.iitk.ac.in/{dept}/courses\",\n",
        "            ]\n",
        "\n",
        "            for url in urls_to_try:\n",
        "                try:\n",
        "                    response = self.session.head(url, timeout=10)\n",
        "                    if response.status_code == 200:\n",
        "                        department_urls.append(url)\n",
        "                        logger.info(f\"Found working department URL: {url}\")\n",
        "                        break  # Found one for this department, move to next\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                time.sleep(0.5)  # Small delay between checks\n",
        "\n",
        "        return department_urls\n",
        "\n",
        "    def scrape_department_pages(self) -> List[Dict]:\n",
        "        \"\"\"Scrape department pages with discovered URLs\"\"\"\n",
        "        department_data = []\n",
        "\n",
        "        # First scrape known working URLs\n",
        "        for url in self.department_urls:\n",
        "            logger.info(f\"Scraping known department URL: {url}\")\n",
        "            page_data = self.scrape_page(url)\n",
        "            if page_data:\n",
        "                department_data.append(page_data)\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Then discover and scrape additional department URLs\n",
        "        logger.info(\"Discovering additional department URLs...\")\n",
        "        discovered_urls = self.discover_department_urls()\n",
        "\n",
        "        for url in discovered_urls:\n",
        "            if url not in self.scraped_urls:\n",
        "                logger.info(f\"Scraping discovered department URL: {url}\")\n",
        "                page_data = self.scrape_page(url)\n",
        "                if page_data:\n",
        "                    department_data.append(page_data)\n",
        "                time.sleep(1)\n",
        "\n",
        "        return department_data\n",
        "\n",
        "    def scrape_from_links(self, start_urls: List[str], max_depth: int = 2) -> List[Dict]:\n",
        "        \"\"\"Scrape following links from initial pages\"\"\"\n",
        "        all_data = []\n",
        "        urls_to_visit = start_urls.copy()\n",
        "        depth = 0\n",
        "\n",
        "        while urls_to_visit and depth < max_depth:\n",
        "            current_level_urls = urls_to_visit.copy()\n",
        "            urls_to_visit = []\n",
        "\n",
        "            for url in current_level_urls:\n",
        "                if url in self.scraped_urls:\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Scraping (depth {depth}): {url}\")\n",
        "                page_data = self.scrape_page(url)\n",
        "\n",
        "                if page_data:\n",
        "                    all_data.append(page_data)\n",
        "\n",
        "                    # Add links from this page for next level\n",
        "                    for link in page_data.get('links', []):\n",
        "                        if link not in self.scraped_urls:\n",
        "                            urls_to_visit.append(link)\n",
        "\n",
        "                time.sleep(1)\n",
        "\n",
        "                # Limit pages per depth level\n",
        "                if len(all_data) > 60:\n",
        "                    break\n",
        "\n",
        "            depth += 1\n",
        "\n",
        "            # Limit total URLs for next level\n",
        "            urls_to_visit = urls_to_visit[:25]\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def scrape_all_sources(self) -> List[Dict]:\n",
        "        \"\"\"Scrape all sources with improved strategy\"\"\"\n",
        "        all_data = []\n",
        "\n",
        "        # 1. Scrape student organizations first (most valuable content)\n",
        "        logger.info(\"=== Scraping Student Organizations ===\")\n",
        "        org_data = self.scrape_student_organizations()\n",
        "        all_data.extend(org_data)\n",
        "        logger.info(f\"Scraped {len(org_data)} pages from student organizations\")\n",
        "\n",
        "        # 2. Scrape main pages\n",
        "        logger.info(\"=== Scraping Main Pages ===\")\n",
        "        for url in self.main_urls:\n",
        "            page_data = self.scrape_page(url)\n",
        "            if page_data:\n",
        "                all_data.append(page_data)\n",
        "            time.sleep(1)\n",
        "\n",
        "        # 3. Scrape known content URLs\n",
        "        logger.info(\"=== Scraping Content Pages ===\")\n",
        "        for url in self.content_urls:\n",
        "            page_data = self.scrape_page(url)\n",
        "            if page_data:\n",
        "                all_data.append(page_data)\n",
        "            time.sleep(1)\n",
        "\n",
        "        # 4. Scrape department pages\n",
        "        logger.info(\"=== Scraping Department Pages ===\")\n",
        "        dept_data = self.scrape_department_pages()\n",
        "        all_data.extend(dept_data)\n",
        "\n",
        "        # 5. Follow links from main pages (limited depth)\n",
        "        logger.info(\"=== Following Links from Main Pages ===\")\n",
        "        link_data = self.scrape_from_links(self.main_urls, max_depth=2)\n",
        "        all_data.extend(link_data)\n",
        "\n",
        "        # 6. Follow links from student organizations\n",
        "        logger.info(\"=== Following Links from Student Organizations ===\")\n",
        "        org_link_data = self.scrape_from_links(self.student_org_urls, max_depth=2)\n",
        "        all_data.extend(org_link_data)\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def save_data(self, data: List[Dict], filename: str = \"enhanced_iitk_data.json\"):\n",
        "        \"\"\"Save scraped data to JSON file with better filtering and organization\"\"\"\n",
        "        # Filter out empty or very short content\n",
        "        filtered_data = []\n",
        "        seen_content = set()\n",
        "\n",
        "        for item in data:\n",
        "            if (item and\n",
        "                item.get('content') and\n",
        "                len(item['content'].split()) > 30 and\n",
        "                len(item['content']) > 250 and\n",
        "                item['content'] not in seen_content):  # Remove duplicates\n",
        "\n",
        "                seen_content.add(item['content'])\n",
        "                filtered_data.append(item)\n",
        "\n",
        "        # Sort by source type and word count\n",
        "        def sort_key(x):\n",
        "            source_priority = {\n",
        "                'student_org': 1,\n",
        "                'placement': 2,\n",
        "                'student_portal': 3,\n",
        "                'official': 4\n",
        "            }\n",
        "            return (source_priority.get(x.get('source_type', 'official'), 5), -x.get('word_count', 0))\n",
        "\n",
        "        filtered_data.sort(key=sort_key)\n",
        "\n",
        "        # Save to JSON\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(filtered_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"Saved {len(filtered_data)} items to {filename}\")\n",
        "\n",
        "        # Also save a simple text version for easy reading\n",
        "        text_filename = filename.replace('.json', '_text.txt')\n",
        "        with open(text_filename, 'w', encoding='utf-8') as f:\n",
        "            for i, item in enumerate(filtered_data):\n",
        "                f.write(f\"=== PAGE {i+1}: {item['title']} ===\\n\")\n",
        "                f.write(f\"URL: {item['url']}\\n\")\n",
        "                f.write(f\"Source Type: {item.get('source_type', 'unknown')}\\n\")\n",
        "                f.write(f\"Words: {item['word_count']}\\n\")\n",
        "                f.write(f\"Content:\\n{item['content']}\\n\\n\")\n",
        "\n",
        "        # Create a separate file for each source type\n",
        "        source_types = {}\n",
        "        for item in filtered_data:\n",
        "            source_type = item.get('source_type', 'unknown')\n",
        "            if source_type not in source_types:\n",
        "                source_types[source_type] = []\n",
        "            source_types[source_type].append(item)\n",
        "\n",
        "        for source_type, items in source_types.items():\n",
        "            source_filename = f\"{source_type}_data.json\"\n",
        "            with open(source_filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(items, f, indent=2, ensure_ascii=False)\n",
        "            logger.info(f\"Saved {len(items)} {source_type} items to {source_filename}\")\n",
        "\n",
        "        # Print detailed statistics\n",
        "        total_words = sum(item.get('word_count', 0) for item in filtered_data)\n",
        "        avg_words = total_words / len(filtered_data) if filtered_data else 0\n",
        "\n",
        "        print(f\"\\n=== ENHANCED DATA COLLECTION SUMMARY ===\")\n",
        "        print(f\"Total pages scraped: {len(filtered_data)}\")\n",
        "        print(f\"Total words: {total_words:,}\")\n",
        "        print(f\"Average words per page: {avg_words:.1f}\")\n",
        "        print(f\"Unique URLs visited: {len(self.scraped_urls)}\")\n",
        "\n",
        "        print(f\"\\n=== BREAKDOWN BY SOURCE TYPE ===\")\n",
        "        for source_type, items in source_types.items():\n",
        "            type_words = sum(item.get('word_count', 0) for item in items)\n",
        "            print(f\"{source_type.replace('_', ' ').title()}: {len(items)} pages, {type_words:,} words\")\n",
        "\n",
        "        print(f\"\\n=== FILES CREATED ===\")\n",
        "        print(f\"Main JSON: {filename}\")\n",
        "        print(f\"Text version: {text_filename}\")\n",
        "        for source_type in source_types:\n",
        "            print(f\"{source_type.replace('_', ' ').title()} JSON: {source_type}_data.json\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "def main():\n",
        "    scraper = EnhancedIITKDataScraper()\n",
        "\n",
        "    print(\"Starting Enhanced IITK Data Scraping...\")\n",
        "    print(\"This will scrape:\")\n",
        "    print(\"- Official IITK websites\")\n",
        "    print(\"- Student organizations: Vox Populi, E-Cell, Gymkhana, AnC Council\")\n",
        "    print(\"- Student Placement Office (SPO)\")\n",
        "    print(\"- Department pages\")\n",
        "    print(\"- And follow relevant links from all sources\")\n",
        "\n",
        "    # Scrape all sources\n",
        "    data = scraper.scrape_all_sources()\n",
        "\n",
        "    # Save to file\n",
        "    filename = scraper.save_data(data)\n",
        "\n",
        "    print(f\"\\nScraping completed successfully!\")\n",
        "\n",
        "    # Show sample of scraped data\n",
        "    if data:\n",
        "        print(f\"\\nSample of scraped content by source type:\")\n",
        "        source_samples = {}\n",
        "        for item in data:\n",
        "            source_type = item.get('source_type', 'unknown')\n",
        "            if source_type not in source_samples:\n",
        "                source_samples[source_type] = item\n",
        "\n",
        "        for source_type, item in source_samples.items():\n",
        "            print(f\"\\n{source_type.replace('_', ' ').title()}:\")\n",
        "            print(f\"  Title: {item['title']}\")\n",
        "            print(f\"  URL: {item['url']}\")\n",
        "            print(f\"  Words: {item['word_count']}\")\n",
        "            print(f\"  Preview: {item['content'][:150]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wuLGSyislKh",
        "outputId": "43b2a5ae-7ee1-476a-871f-dc2948522055"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok -q\n",
        "\n",
        "app_code = '''\n",
        "import streamlit as st\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import logging\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class IITKChatbot:\n",
        "    def __init__(self, data_file: str = \"enhanced_iitk_data.json\"):\n",
        "        self.data_file = data_file\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "        self.embedding_model = None\n",
        "        self.qa_pipeline = None\n",
        "        self.tokenizer = None\n",
        "        self.qa_model = None\n",
        "\n",
        "        # Initialize the chatbot\n",
        "        self.initialize_chatbot()\n",
        "\n",
        "    def initialize_chatbot(self):\n",
        "        \"\"\"Initialize the complete chatbot system\"\"\"\n",
        "        try:\n",
        "            self.load_data()\n",
        "            self.initialize_models()\n",
        "            self.create_embeddings()\n",
        "            logger.info(\"Chatbot initialization completed successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during chatbot initialization: {str(e)}\")\n",
        "            st.error(f\"Failed to initialize chatbot: {str(e)}\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load scraped data from JSON file\"\"\"\n",
        "        try:\n",
        "            with open(self.data_file, 'r', encoding='utf-8') as f:\n",
        "                raw_data = json.load(f)\n",
        "\n",
        "            # Process documents\n",
        "            for item in raw_data:\n",
        "                if item.get('content') and len(item['content'].split()) > 20:\n",
        "                    # Split long content into chunks\n",
        "                    chunks = self.split_into_chunks(item['content'])\n",
        "                    for chunk in chunks:\n",
        "                        self.documents.append({\n",
        "                            'title': item.get('title', 'No Title'),\n",
        "                            'content': chunk,\n",
        "                            'url': item.get('url', ''),\n",
        "                            'source_type': item.get('source_type', 'unknown'),\n",
        "                            'sections': item.get('sections', [])\n",
        "                        })\n",
        "\n",
        "            logger.info(f\"Loaded {len(self.documents)} document chunks\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(f\"Data file {self.data_file} not found! Using sample data.\")\n",
        "            # Create comprehensive sample data for demonstration\n",
        "            self.documents = [\n",
        "                {\n",
        "                    'title': 'IIT Kanpur Overview',\n",
        "                    'content': 'Indian Institute of Technology Kanpur (IIT Kanpur) is one of the premier engineering institutions in India. Established in 1959, it was the first IIT to be set up with foreign assistance. The institute offers undergraduate, postgraduate, and doctoral programs in engineering, science, design, and management. IIT Kanpur is located in Kanpur, Uttar Pradesh, and spans over 1055 acres. It is consistently ranked among the top engineering institutions in India.',\n",
        "                    'url': 'https://www.iitk.ac.in/',\n",
        "                    'source_type': 'official',\n",
        "                    'sections': ['About', 'History']\n",
        "                },\n",
        "                {\n",
        "                    'title': 'Academic Programs at IIT Kanpur',\n",
        "                    'content': 'IIT Kanpur offers various academic programs including Bachelor of Technology (B.Tech), Master of Technology (M.Tech), Master of Science (M.S.), and Doctor of Philosophy (Ph.D.). The institute has 16 academic departments covering engineering disciplines like Computer Science, Mechanical Engineering, Electrical Engineering, Civil Engineering, Chemical Engineering, and Aerospace Engineering. It also offers programs in Mathematics, Physics, Chemistry, Humanities and Social Sciences, and Management.',\n",
        "                    'url': 'https://www.iitk.ac.in/academics',\n",
        "                    'source_type': 'official',\n",
        "                    'sections': ['Programs', 'Departments']\n",
        "                },\n",
        "                {\n",
        "                    'title': 'Student Life at IIT Kanpur',\n",
        "                    'content': 'Student life at IIT Kanpur is vibrant and diverse. The campus has 12 halls of residence (hostels) accommodating over 8000 students. The institute has numerous student clubs and societies including technical clubs, cultural clubs, and sports clubs. Major festivals include Antaragni (cultural festival), Techkriti (technical festival), and Udghosh (sports festival). The Students Gymkhana is the student government body that organizes various activities and represents student interests.',\n",
        "                    'url': 'https://students.iitk.ac.in/',\n",
        "                    'source_type': 'student_portal',\n",
        "                    'sections': ['Hostels', 'Clubs', 'Festivals']\n",
        "                },\n",
        "                {\n",
        "                    'title': 'Research and Innovation',\n",
        "                    'content': 'IIT Kanpur is renowned for its research contributions in various fields. The institute has established several centers of excellence including the National Centre for Flexible Electronics, Advanced Centre for Materials Science, and the National Wind Tunnel Facility. Faculty and students engage in cutting-edge research in areas like artificial intelligence, robotics, nanotechnology, biotechnology, and renewable energy. The institute has strong industry partnerships and encourages innovation and entrepreneurship.',\n",
        "                    'url': 'https://www.iitk.ac.in/research',\n",
        "                    'source_type': 'official',\n",
        "                    'sections': ['Research Areas', 'Centers', 'Innovation']\n",
        "                },\n",
        "                {\n",
        "                    'title': 'Placement and Career Services',\n",
        "                    'content': 'The Student Placement Office (SPO) at IIT Kanpur facilitates campus placements for students. Top companies from various sectors including IT, consulting, finance, and core engineering visit the campus for recruitment. The average package for B.Tech students is around 15-20 LPA, while for M.Tech and Ph.D. students, it varies based on specialization. The institute has a strong alumni network working in top positions across industries globally.',\n",
        "                    'url': 'https://spo.iitk.ac.in/',\n",
        "                    'source_type': 'placement',\n",
        "                    'sections': ['Placements', 'Companies', 'Statistics']\n",
        "                },\n",
        "                {\n",
        "                    'title': 'Campus Facilities',\n",
        "                    'content': 'IIT Kanpur campus provides excellent facilities including modern laboratories, libraries, sports facilities, and recreational areas. The P.K. Kelkar Library is one of the largest technical libraries in India. The campus has a health center, guest house, shopping complex, and multiple dining facilities. Sports facilities include swimming pool, gymnasium, tennis courts, football ground, and cricket ground. The campus is Wi-Fi enabled and provides 24/7 internet connectivity.',\n",
        "                    'url': 'https://www.iitk.ac.in/facilities',\n",
        "                    'source_type': 'official',\n",
        "                    'sections': ['Library', 'Sports', 'Health', 'Dining']\n",
        "                }\n",
        "            ]\n",
        "\n",
        "    def split_into_chunks(self, text: str, max_length: int = 400) -> List[str]:\n",
        "        \"\"\"Split text into manageable chunks for better processing\"\"\"\n",
        "        # First try to split by sentences\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            # Check if adding this sentence would exceed max_length\n",
        "            if len(current_chunk.split()) + len(sentence.split()) <= max_length:\n",
        "                current_chunk += sentence + \". \"\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence + \". \"\n",
        "\n",
        "        # Add the last chunk if it exists\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        # If no chunks were created (very long sentences), split by words\n",
        "        if not chunks:\n",
        "            words = text.split()\n",
        "            for i in range(0, len(words), max_length):\n",
        "                chunk = ' '.join(words[i:i + max_length])\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize transformer models with better error handling\"\"\"\n",
        "        try:\n",
        "            # Initialize sentence transformer for embeddings\n",
        "            st.info(\"Loading sentence transformer model...\")\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "            # Initialize QA pipeline with a lightweight model\n",
        "            st.info(\"Loading question-answering model...\")\n",
        "            model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "            self.qa_pipeline = pipeline(\n",
        "                \"question-answering\",\n",
        "                model=model_name,\n",
        "                tokenizer=model_name,\n",
        "                device=-1  # Force CPU usage\n",
        "            )\n",
        "\n",
        "            # Also load tokenizer and model separately for more control\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "            logger.info(\"Models initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing models: {str(e)}\")\n",
        "            st.error(f\"Failed to initialize models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_embeddings(self):\n",
        "        \"\"\"Create embeddings for all documents using sentence transformers\"\"\"\n",
        "        if not self.embedding_model or not self.documents:\n",
        "            logger.warning(\"Cannot create embeddings: missing model or documents\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            st.info(\"Creating document embeddings...\")\n",
        "\n",
        "            # Create embeddings for all document contents\n",
        "            texts = [doc['content'] for doc in self.documents]\n",
        "\n",
        "            # Create embeddings in batches to avoid memory issues\n",
        "            batch_size = 32\n",
        "            all_embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                batch_embeddings = self.embedding_model.encode(\n",
        "                    batch_texts,\n",
        "                    convert_to_tensor=False,\n",
        "                    show_progress_bar=False\n",
        "                )\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "            self.embeddings = np.array(all_embeddings)\n",
        "            logger.info(f\"Created embeddings for {len(self.documents)} documents\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating embeddings: {str(e)}\")\n",
        "            st.error(f\"Failed to create embeddings: {str(e)}\")\n",
        "\n",
        "    def find_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Find most relevant documents for a query using cosine similarity\"\"\"\n",
        "        if not self.embedding_model or self.embeddings is None:\n",
        "            logger.warning(\"Using fallback document selection\")\n",
        "            return self.documents[:top_k]\n",
        "\n",
        "        try:\n",
        "            # Encode query\n",
        "            query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "\n",
        "            # Get top-k most similar documents\n",
        "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "            # Return relevant documents with scores\n",
        "            relevant_docs = []\n",
        "            for idx in top_indices:\n",
        "                if idx < len(self.documents):\n",
        "                    doc = self.documents[idx].copy()\n",
        "                    doc['relevance_score'] = float(similarities[idx])\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error finding relevant documents: {str(e)}\")\n",
        "            # Fallback to simple keyword matching\n",
        "            return self.simple_keyword_search(query, top_k)\n",
        "\n",
        "    def simple_keyword_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Fallback keyword-based search\"\"\"\n",
        "        query_words = set(query.lower().split())\n",
        "        scored_docs = []\n",
        "\n",
        "        for doc in self.documents:\n",
        "            content_words = set(doc['content'].lower().split())\n",
        "            title_words = set(doc['title'].lower().split())\n",
        "\n",
        "            # Calculate simple overlap score\n",
        "            content_score = len(query_words.intersection(content_words))\n",
        "            title_score = len(query_words.intersection(title_words)) * 2  # Weight title matches more\n",
        "\n",
        "            total_score = content_score + title_score\n",
        "\n",
        "            if total_score > 0:\n",
        "                doc_copy = doc.copy()\n",
        "                doc_copy['relevance_score'] = total_score\n",
        "                scored_docs.append(doc_copy)\n",
        "\n",
        "        # Sort by score and return top-k\n",
        "        scored_docs.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        return scored_docs[:top_k]\n",
        "\n",
        "    def answer_question(self, question: str) -> Dict:\n",
        "        \"\"\"Answer a question using the QA pipeline\"\"\"\n",
        "        if not self.qa_pipeline:\n",
        "            return {\n",
        "                'answer': \"Sorry, the QA model is not available.\",\n",
        "                'confidence': 0.0,\n",
        "                'context': \"\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Find relevant documents\n",
        "            relevant_docs = self.find_relevant_documents(question, top_k=3)\n",
        "\n",
        "            if not relevant_docs:\n",
        "                return {\n",
        "                    'answer': \"I couldn't find relevant information to answer your question about IIT Kanpur.\",\n",
        "                    'confidence': 0.0,\n",
        "                    'context': \"\",\n",
        "                    'sources': []\n",
        "                }\n",
        "\n",
        "            # Combine contexts from relevant documents\n",
        "            contexts = []\n",
        "            for doc in relevant_docs:\n",
        "                contexts.append(doc['content'])\n",
        "\n",
        "            # Try each context separately and pick the best answer\n",
        "            best_answer = None\n",
        "            best_confidence = 0.0\n",
        "            best_context = \"\"\n",
        "\n",
        "            for context in contexts:\n",
        "                # Truncate context if too long for the model\n",
        "                if len(context) > 2000:\n",
        "                    context = context[:2000]\n",
        "\n",
        "                try:\n",
        "                    result = self.qa_pipeline(question=question, context=context)\n",
        "\n",
        "                    if result['score'] > best_confidence:\n",
        "                        best_answer = result['answer']\n",
        "                        best_confidence = result['score']\n",
        "                        best_context = context\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error processing context: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            # If no answer found, try with combined context\n",
        "            if not best_answer:\n",
        "                combined_context = \" \".join(contexts)\n",
        "                if len(combined_context) > 2000:\n",
        "                    combined_context = combined_context[:2000]\n",
        "\n",
        "                try:\n",
        "                    result = self.qa_pipeline(question=question, context=combined_context)\n",
        "                    best_answer = result['answer']\n",
        "                    best_confidence = result['score']\n",
        "                    best_context = combined_context\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error with combined context: {str(e)}\")\n",
        "                    best_answer = \"I found some information but couldn't extract a specific answer.\"\n",
        "                    best_confidence = 0.1\n",
        "\n",
        "            # Extract sources\n",
        "            sources = []\n",
        "            for doc in relevant_docs:\n",
        "                sources.append({\n",
        "                    'title': doc['title'],\n",
        "                    'url': doc['url'],\n",
        "                    'source_type': doc.get('source_type', 'unknown'),\n",
        "                    'relevance': doc.get('relevance_score', 0.0)\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                'answer': best_answer,\n",
        "                'confidence': best_confidence,\n",
        "                'context': best_context,\n",
        "                'sources': sources\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error answering question: {str(e)}\")\n",
        "            return {\n",
        "                'answer': f\"I encountered an error while processing your question. Please try rephrasing it.\",\n",
        "                'confidence': 0.0,\n",
        "                'context': \"\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"IIT Kanpur Chatbot\",\n",
        "        page_icon=\"🤖\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "\n",
        "    # Custom CSS for better styling\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .main-header {\n",
        "        background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);\n",
        "        padding: 1rem;\n",
        "        border-radius: 10px;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .main-header h1 {\n",
        "        color: white;\n",
        "        margin: 0;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .main-header p {\n",
        "        color: #e0e0e0;\n",
        "        margin: 0;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .chat-container {\n",
        "        border: 1px solid #ddd;\n",
        "        border-radius: 10px;\n",
        "        padding: 1rem;\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "    .source-box {\n",
        "        background-color: #f8f9fa;\n",
        "        border-left: 4px solid #007bff;\n",
        "        padding: 0.5rem;\n",
        "        margin: 0.5rem 0;\n",
        "        border-radius: 0 5px 5px 0;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Header\n",
        "    st.markdown(\"\"\"\n",
        "    <div class=\"main-header\">\n",
        "        <h1>IIT Kanpur Chatbot</h1>\n",
        "        <p>An AI-powered chatbot to answer questions about IIT Kanpur</p>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Initialize chatbot\n",
        "    if 'chatbot' not in st.session_state:\n",
        "        with st.spinner(\"Initializing PULPNET chatbot... This may take a moment.\"):\n",
        "            try:\n",
        "                st.session_state.chatbot = IITKChatbot()\n",
        "                st.success(\"PULPNET is ready to help!\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Failed to initialize chatbot: {str(e)}\")\n",
        "                st.stop()\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.header(\"About IITK ChatBot\")\n",
        "        st.info(\n",
        "            \"PULPNET is an AI-powered chatbot designed to answer questions about IIT Kanpur. \"\n",
        "            \"It uses advanced transformer models to provide accurate and helpful responses based on \"\n",
        "            \"official and student-led information sources.\"\n",
        "        )\n",
        "\n",
        "        st.header(\"Technical Details\")\n",
        "        st.write(\"**Embedding Model:** all-MiniLM-L6-v2\")\n",
        "        st.write(\"**QA Model:** DistilBERT-base-cased\")\n",
        "        st.write(\"**Search Method:** Cosine Similarity\")\n",
        "\n",
        "        # Statistics\n",
        "        if hasattr(st.session_state.chatbot, 'documents'):\n",
        "            st.header(\"Dataset Statistics\")\n",
        "            st.metric(\"Total Documents\", len(st.session_state.chatbot.documents))\n",
        "\n",
        "            # Show source types\n",
        "            source_types = {}\n",
        "            for doc in st.session_state.chatbot.documents:\n",
        "                source_type = doc.get('source_type', 'unknown')\n",
        "                source_types[source_type] = source_types.get(source_type, 0) + 1\n",
        "\n",
        "            st.write(\"**Sources:**\")\n",
        "            for source_type, count in source_types.items():\n",
        "                st.write(f\"• {source_type.replace('_', ' ').title()}: {count}\")\n",
        "\n",
        "        # Sample questions\n",
        "        st.header(\"Sample Questions\")\n",
        "        sample_questions = [\n",
        "            \"What is IIT Kanpur?\",\n",
        "            \"What academic programs are offered?\",\n",
        "            \"Tell me about student life\",\n",
        "            \"What research areas are there?\",\n",
        "            \"How is the placement scenario?\",\n",
        "            \"What facilities are available on campus?\",\n",
        "            \"Tell me about the hostels\",\n",
        "            \"What are the major festivals?\"\n",
        "        ]\n",
        "\n",
        "        for question in sample_questions:\n",
        "            if st.button(question, key=f\"sample_{question}\", use_container_width=True):\n",
        "                st.session_state.sample_question = question\n",
        "                st.rerun()\n",
        "\n",
        "    # Main chat interface\n",
        "    st.subheader(\"Ask IITK ChatBot\")\n",
        "\n",
        "    # Input methods\n",
        "    col1, col2 = st.columns([3, 1])\n",
        "    with col1:\n",
        "        user_question = st.text_input(\n",
        "            \"Enter your question about IIT Kanpur:\",\n",
        "            placeholder=\"e.g., What are the academic programs at IIT Kanpur?\",\n",
        "            key=\"user_input\"\n",
        "        )\n",
        "    with col2:\n",
        "        ask_button = st.button(\"Ask Question\", type=\"primary\", use_container_width=True)\n",
        "\n",
        "    # Handle sample question\n",
        "    if 'sample_question' in st.session_state:\n",
        "        user_question = st.session_state.sample_question\n",
        "        ask_button = True\n",
        "        del st.session_state.sample_question\n",
        "\n",
        "    # Process question\n",
        "    if ask_button and user_question:\n",
        "        with st.spinner(\"IITK ChatBot is thinking...\"):\n",
        "            response = st.session_state.chatbot.answer_question(user_question)\n",
        "\n",
        "        # Display results\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "        # Answer section\n",
        "        st.subheader(\"📝 Answer\")\n",
        "\n",
        "        # Show confidence level with color coding\n",
        "        confidence = response['confidence']\n",
        "        if confidence > 0.7:\n",
        "            confidence_color = \"green\"\n",
        "            confidence_text = \"High\"\n",
        "        elif confidence > 0.4:\n",
        "            confidence_color = \"orange\"\n",
        "            confidence_text = \"Medium\"\n",
        "        else:\n",
        "            confidence_color = \"red\"\n",
        "            confidence_text = \"Low\"\n",
        "\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "        with col1:\n",
        "            st.markdown(f\"**{response['answer']}**\")\n",
        "        with col2:\n",
        "            st.markdown(f\"**Confidence:** <span style='color:{confidence_color}'>{confidence_text} ({confidence:.2%})</span>\", unsafe_allow_html=True)\n",
        "\n",
        "        # Sources section\n",
        "        if response['sources']:\n",
        "            st.subheader(\"Sources\")\n",
        "            for i, source in enumerate(response['sources']):\n",
        "                st.markdown(f\"\"\"\n",
        "                <div class=\"source-box\">\n",
        "                    <strong>{source['title']}</strong><br>\n",
        "                    <small>Type: {source['source_type'].replace('_', ' ').title()} |\n",
        "                    Relevance: {source['relevance']:.2f}</small><br>\n",
        "                    <a href=\"{source['url']}\" target=\"_blank\">{source['url']}</a>\n",
        "                </div>\n",
        "                \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Context section (expandable)\n",
        "        if response['context']:\n",
        "            with st.expander(\"Context Used (Click to expand)\"):\n",
        "                st.text_area(\"Context\", response['context'], height=200, disabled=True)\n",
        "\n",
        "    elif ask_button and not user_question:\n",
        "        st.warning(\"Please enter a question before clicking 'Ask Question'.\")\n",
        "\n",
        "    # Footer\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"\"\"\n",
        "    <div style=\"text-align: center; color: #666;\">\n",
        "        <p>IIT Kanpur Chatbot | Built with using Streamlit and Transformers</p>\n",
        "        <p><small>For the best experience, ask specific questions about IIT Kanpur academics, facilities, student life, or research.</small></p>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"app.py has been created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_efSPl1iLH7",
        "outputId": "5cb4e2af-31c0-4de6-d298-49db0ebe00db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py has been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Get your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok.set_auth_token(\"2zTWsXhD47dWMWJv4jHORDOoZia_5Qmo6pxYw18yokxW1bvC2\")\n",
        "\n",
        "!nohup streamlit run app.py --server.port 8501 &\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"IITK ChatBot Streamlit app is live! Click here: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKQRr_nLv5Xi",
        "outputId": "06e7092d-56ac-4aa6-b8df-1deb7f1aba23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "IITK ChatBot Streamlit app is live! Click here: NgrokTunnel: \"https://420e-34-63-80-175.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkS39r3_xjXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}